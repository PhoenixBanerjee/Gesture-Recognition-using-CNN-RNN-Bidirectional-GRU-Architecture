{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jegtlUv6o6Y1"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nsdaWDTmo6Y6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread, imresize\n",
    "#from scipy.imageio import imread, imresize\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCz5iBNao6Y9"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h103dcrZo6Y-"
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmFUmI2No6Y_"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2U42ONlfo6Y_"
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(r'C:\\Users\\kaush\\0_Upgrad\\Deep Learning\\Gesture Recognition\\Project_data\\train.csv').readlines())\n",
    "\n",
    "val_doc = np.random.permutation(open(r'C:\\Users\\kaush\\0_Upgrad\\Deep Learning\\Gesture Recognition\\Project_data\\val.csv').readlines())\n",
    "\n",
    "batch_size = 32 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFoPVr7_o6ZA"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VW5_kKW5o6ZB"
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,1,2,5,7,8,10,13,16,18,20,21,24,25,27,29] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size) # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    height, width, channel = image.shape\n",
    "                    if height == 120 or width == 120:\n",
    "                      image=image[20:140,:120,:]\n",
    "                    image = resize(image, (100, 100, 3)).astype('float32')/255.  \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if len(folder_list) > num_batches * batch_size:\n",
    "          remaining_data_size = len(folder_list) - (num_batches * batch_size)\n",
    "          batch_data = np.zeros((remaining_data_size,len(img_idx),100,100,3))\n",
    "          batch_labels = np.zeros((remaining_data_size,5)) # batch_labels is the one hot representation of the output\n",
    "          for folder in range(remaining_data_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    height, width, channel = image.shape\n",
    "                    if height == 120 or width == 120:\n",
    "                      image=image[20:140,:120,:]\n",
    "                    image = resize(image, (100, 100, 3)).astype('float32')/255.  \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "          yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh7DCjJko6ZD"
   },
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9w_MqSdo6ZE",
    "outputId": "41d622a2-007a-4ee4-dd5e-d531146c03c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = r'C:\\Users\\kaush\\0_Upgrad\\Deep Learning\\Gesture Recognition\\Project_data\\train'\n",
    "val_path = r'C:\\Users\\kaush\\0_Upgrad\\Deep Learning\\Gesture Recognition\\Project_data\\val'\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 100 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38qUWoW2o6ZF"
   },
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q_tjSWoao6ZF"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, Bidirectional\n",
    "from keras.layers.convolutional import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#write your model here\n",
    "num_classes = 5\n",
    "cnn_model = Sequential([\n",
    "  layers.Conv2D(16, (3,3), padding='same', input_shape=(100,100,3), activation='relu'),\n",
    "  layers.BatchNormalization(),\n",
    "  layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  layers.Conv2D(32, (3,3), padding='same', input_shape=(100,100,3), activation='relu'),\n",
    "  layers.BatchNormalization(),\n",
    "  layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  layers.Conv2D(64, (3,3), padding='same', input_shape=(100,100,3), activation='relu'),\n",
    "  layers.BatchNormalization(),\n",
    "  layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  layers.Conv2D(128, (3,3), padding='same', input_shape=(100,100,3), activation='relu'),\n",
    "  layers.BatchNormalization(),\n",
    "  layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dropout(0.3),\n",
    "  layers.Dense(256, activation='relu')\n",
    "])\n",
    "model = Sequential([  \n",
    "  layers.TimeDistributed(cnn_model, input_shape=(16,100,100,3)),\n",
    "  layers.Bidirectional(GRU(16)),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI6oAsTLo6ZF"
   },
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9hpIDPl5o6ZG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimiser = 'adam' #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrDAGxkDPIxV",
    "outputId": "7a8ff1ec-7f86-4e6f-a44a-b183503f2e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 16, 256)          1344096   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 32)               26304     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,370,565\n",
      "Trainable params: 1,370,085\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EpRGe61o6ZG"
   },
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5zTIrorvo6ZH"
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44CHdqZ3o6ZH",
    "outputId": "1cf33c07-3284-4187-f5a4-f8a776e5b4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.0001) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K65sDZMio6ZH"
   },
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kgKjYLwmo6ZI"
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBPv5c9no6ZI"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "CrnKip3Oo6ZJ",
    "outputId": "bf73cd5a-e0a5-4d50-f904-aa91bfaef3d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\Users\\kaush\\0_Upgrad\\Deep Learning\\Gesture Recognition\\Project_data\\train ; batch size = 32\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6558 - categorical_accuracy: 0.2821Source path =  C:\\Users\\kaush\\0_Upgrad\\Deep Learning\\Gesture Recognition\\Project_data\\val ; batch size = 32\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-07-0401_18_36.432528\\model-00001-1.65582-0.28205-1.76029-0.24000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 1.6558 - categorical_accuracy: 0.2821 - val_loss: 1.7603 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2516 - categorical_accuracy: 0.4781\n",
      "Epoch 2: saving model to model_init_2022-07-0401_18_36.432528\\model-00002-1.25158-0.47813-1.71257-0.21000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 1.2516 - categorical_accuracy: 0.4781 - val_loss: 1.7126 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0157 - categorical_accuracy: 0.6275\n",
      "Epoch 3: saving model to model_init_2022-07-0401_18_36.432528\\model-00003-1.01567-0.62745-2.10095-0.23000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 1.0157 - categorical_accuracy: 0.6275 - val_loss: 2.1010 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9015 - categorical_accuracy: 0.6546\n",
      "Epoch 4: saving model to model_init_2022-07-0401_18_36.432528\\model-00004-0.90151-0.65460-2.07105-0.14000.h5\n",
      "21/21 [==============================] - 181s 9s/step - loss: 0.9015 - categorical_accuracy: 0.6546 - val_loss: 2.0710 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7457 - categorical_accuracy: 0.7602\n",
      "Epoch 5: saving model to model_init_2022-07-0401_18_36.432528\\model-00005-0.74572-0.76018-1.98222-0.20000.h5\n",
      "21/21 [==============================] - 174s 8s/step - loss: 0.7457 - categorical_accuracy: 0.7602 - val_loss: 1.9822 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7152 - categorical_accuracy: 0.7692\n",
      "Epoch 6: saving model to model_init_2022-07-0401_18_36.432528\\model-00006-0.71516-0.76923-2.08522-0.26000.h5\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.7152 - categorical_accuracy: 0.7692 - val_loss: 2.0852 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5475 - categorical_accuracy: 0.8205\n",
      "Epoch 7: saving model to model_init_2022-07-0401_18_36.432528\\model-00007-0.54748-0.82051-2.02101-0.32000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.5475 - categorical_accuracy: 0.8205 - val_loss: 2.0210 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5393 - categorical_accuracy: 0.8130\n",
      "Epoch 8: saving model to model_init_2022-07-0401_18_36.432528\\model-00008-0.53932-0.81297-1.90250-0.33000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.5393 - categorical_accuracy: 0.8130 - val_loss: 1.9025 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4744 - categorical_accuracy: 0.8401\n",
      "Epoch 9: saving model to model_init_2022-07-0401_18_36.432528\\model-00009-0.47436-0.84012-1.92861-0.40000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.4744 - categorical_accuracy: 0.8401 - val_loss: 1.9286 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3999 - categorical_accuracy: 0.8763\n",
      "Epoch 10: saving model to model_init_2022-07-0401_18_36.432528\\model-00010-0.39986-0.87632-2.13489-0.26000.h5\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.3999 - categorical_accuracy: 0.8763 - val_loss: 2.1349 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3223 - categorical_accuracy: 0.9050\n",
      "Epoch 11: saving model to model_init_2022-07-0401_18_36.432528\\model-00011-0.32234-0.90498-2.05703-0.44000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.3223 - categorical_accuracy: 0.9050 - val_loss: 2.0570 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2458 - categorical_accuracy: 0.9367\n",
      "Epoch 12: saving model to model_init_2022-07-0401_18_36.432528\\model-00012-0.24580-0.93665-1.68029-0.49000.h5\n",
      "21/21 [==============================] - 160s 8s/step - loss: 0.2458 - categorical_accuracy: 0.9367 - val_loss: 1.6803 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2787 - categorical_accuracy: 0.9201\n",
      "Epoch 13: saving model to model_init_2022-07-0401_18_36.432528\\model-00013-0.27866-0.92006-1.92858-0.39000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.2787 - categorical_accuracy: 0.9201 - val_loss: 1.9286 - val_categorical_accuracy: 0.3900 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2321 - categorical_accuracy: 0.9397\n",
      "Epoch 14: saving model to model_init_2022-07-0401_18_36.432528\\model-00014-0.23211-0.93967-0.79619-0.69000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.2321 - categorical_accuracy: 0.9397 - val_loss: 0.7962 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2065 - categorical_accuracy: 0.9487\n",
      "Epoch 15: saving model to model_init_2022-07-0401_18_36.432528\\model-00015-0.20648-0.94872-0.83726-0.75000.h5\n",
      "21/21 [==============================] - 160s 8s/step - loss: 0.2065 - categorical_accuracy: 0.9487 - val_loss: 0.8373 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1403 - categorical_accuracy: 0.9698\n",
      "Epoch 16: saving model to model_init_2022-07-0401_18_36.432528\\model-00016-0.14033-0.96983-0.68251-0.77000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.1403 - categorical_accuracy: 0.9698 - val_loss: 0.6825 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1052 - categorical_accuracy: 0.9804\n",
      "Epoch 17: saving model to model_init_2022-07-0401_18_36.432528\\model-00017-0.10521-0.98039-0.68920-0.79000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.1052 - categorical_accuracy: 0.9804 - val_loss: 0.6892 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1015 - categorical_accuracy: 0.9819\n",
      "Epoch 18: saving model to model_init_2022-07-0401_18_36.432528\\model-00018-0.10151-0.98190-0.49913-0.89000.h5\n",
      "21/21 [==============================] - 170s 8s/step - loss: 0.1015 - categorical_accuracy: 0.9819 - val_loss: 0.4991 - val_categorical_accuracy: 0.8900 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0980 - categorical_accuracy: 0.9834\n",
      "Epoch 19: saving model to model_init_2022-07-0401_18_36.432528\\model-00019-0.09795-0.98341-0.69426-0.78000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0980 - categorical_accuracy: 0.9834 - val_loss: 0.6943 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0705 - categorical_accuracy: 0.9894\n",
      "Epoch 20: saving model to model_init_2022-07-0401_18_36.432528\\model-00020-0.07052-0.98944-0.57765-0.85000.h5\n",
      "21/21 [==============================] - 155s 7s/step - loss: 0.0705 - categorical_accuracy: 0.9894 - val_loss: 0.5777 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0659 - categorical_accuracy: 0.9864\n",
      "Epoch 21: saving model to model_init_2022-07-0401_18_36.432528\\model-00021-0.06588-0.98643-0.52586-0.83000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0659 - categorical_accuracy: 0.9864 - val_loss: 0.5259 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0894 - categorical_accuracy: 0.9774\n",
      "Epoch 22: saving model to model_init_2022-07-0401_18_36.432528\\model-00022-0.08939-0.97738-0.46515-0.82000.h5\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.0894 - categorical_accuracy: 0.9774 - val_loss: 0.4652 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0837 - categorical_accuracy: 0.9834\n",
      "Epoch 23: saving model to model_init_2022-07-0401_18_36.432528\\model-00023-0.08368-0.98341-0.69638-0.81000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.0837 - categorical_accuracy: 0.9834 - val_loss: 0.6964 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0957 - categorical_accuracy: 0.9834\n",
      "Epoch 24: saving model to model_init_2022-07-0401_18_36.432528\\model-00024-0.09566-0.98341-0.60232-0.82000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0957 - categorical_accuracy: 0.9834 - val_loss: 0.6023 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0776 - categorical_accuracy: 0.9804\n",
      "Epoch 25: saving model to model_init_2022-07-0401_18_36.432528\\model-00025-0.07760-0.98039-0.62824-0.79000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0776 - categorical_accuracy: 0.9804 - val_loss: 0.6282 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0695 - categorical_accuracy: 0.9910\n",
      "Epoch 26: saving model to model_init_2022-07-0401_18_36.432528\\model-00026-0.06947-0.99095-0.60257-0.81000.h5\n",
      "21/21 [==============================] - 160s 8s/step - loss: 0.0695 - categorical_accuracy: 0.9910 - val_loss: 0.6026 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0464 - categorical_accuracy: 0.9940\n",
      "Epoch 27: saving model to model_init_2022-07-0401_18_36.432528\\model-00027-0.04641-0.99397-0.44862-0.85000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.0464 - categorical_accuracy: 0.9940 - val_loss: 0.4486 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0428 - categorical_accuracy: 0.9925\n",
      "Epoch 28: saving model to model_init_2022-07-0401_18_36.432528\\model-00028-0.04285-0.99246-0.43713-0.84000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0428 - categorical_accuracy: 0.9925 - val_loss: 0.4371 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0386 - categorical_accuracy: 0.9940\n",
      "Epoch 29: saving model to model_init_2022-07-0401_18_36.432528\\model-00029-0.03857-0.99397-0.50835-0.83000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.0386 - categorical_accuracy: 0.9940 - val_loss: 0.5083 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0259 - categorical_accuracy: 1.0000\n",
      "Epoch 30: saving model to model_init_2022-07-0401_18_36.432528\\model-00030-0.02586-1.00000-0.45242-0.88000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0259 - categorical_accuracy: 1.0000 - val_loss: 0.4524 - val_categorical_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0302 - categorical_accuracy: 0.9970\n",
      "Epoch 31: saving model to model_init_2022-07-0401_18_36.432528\\model-00031-0.03018-0.99698-0.47375-0.88000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0302 - categorical_accuracy: 0.9970 - val_loss: 0.4737 - val_categorical_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0268 - categorical_accuracy: 0.9955\n",
      "Epoch 32: saving model to model_init_2022-07-0401_18_36.432528\\model-00032-0.02677-0.99548-0.46649-0.86000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0268 - categorical_accuracy: 0.9955 - val_loss: 0.4665 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0271 - categorical_accuracy: 0.9955\n",
      "Epoch 33: saving model to model_init_2022-07-0401_18_36.432528\\model-00033-0.02713-0.99548-0.67620-0.74000.h5\n",
      "21/21 [==============================] - 169s 8s/step - loss: 0.0271 - categorical_accuracy: 0.9955 - val_loss: 0.6762 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0652 - categorical_accuracy: 0.9849\n",
      "Epoch 34: saving model to model_init_2022-07-0401_18_36.432528\\model-00034-0.06516-0.98492-1.15634-0.65000.h5\n",
      "21/21 [==============================] - 164s 8s/step - loss: 0.0652 - categorical_accuracy: 0.9849 - val_loss: 1.1563 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0378 - categorical_accuracy: 0.9940\n",
      "Epoch 35: saving model to model_init_2022-07-0401_18_36.432528\\model-00035-0.03776-0.99397-0.60679-0.80000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0378 - categorical_accuracy: 0.9940 - val_loss: 0.6068 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0223 - categorical_accuracy: 0.9985\n",
      "Epoch 36: saving model to model_init_2022-07-0401_18_36.432528\\model-00036-0.02233-0.99849-0.43722-0.82000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0223 - categorical_accuracy: 0.9985 - val_loss: 0.4372 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0264 - categorical_accuracy: 0.9970\n",
      "Epoch 37: saving model to model_init_2022-07-0401_18_36.432528\\model-00037-0.02640-0.99698-0.55465-0.85000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.0264 - categorical_accuracy: 0.9970 - val_loss: 0.5546 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0159 - categorical_accuracy: 0.9985\n",
      "Epoch 38: saving model to model_init_2022-07-0401_18_36.432528\\model-00038-0.01594-0.99849-0.49107-0.86000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0159 - categorical_accuracy: 0.9985 - val_loss: 0.4911 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0136 - categorical_accuracy: 1.0000\n",
      "Epoch 39: saving model to model_init_2022-07-0401_18_36.432528\\model-00039-0.01362-1.00000-0.39647-0.89000.h5\n",
      "21/21 [==============================] - 160s 8s/step - loss: 0.0136 - categorical_accuracy: 1.0000 - val_loss: 0.3965 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0172 - categorical_accuracy: 0.9970\n",
      "Epoch 40: saving model to model_init_2022-07-0401_18_36.432528\\model-00040-0.01721-0.99698-0.41718-0.88000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.0172 - categorical_accuracy: 0.9970 - val_loss: 0.4172 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0128 - categorical_accuracy: 1.0000\n",
      "Epoch 41: saving model to model_init_2022-07-0401_18_36.432528\\model-00041-0.01280-1.00000-0.45750-0.87000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.0128 - categorical_accuracy: 1.0000 - val_loss: 0.4575 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0125 - categorical_accuracy: 1.0000\n",
      "Epoch 42: saving model to model_init_2022-07-0401_18_36.432528\\model-00042-0.01254-1.00000-0.60749-0.84000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0125 - categorical_accuracy: 1.0000 - val_loss: 0.6075 - val_categorical_accuracy: 0.8400 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0117 - categorical_accuracy: 1.0000\n",
      "Epoch 43: saving model to model_init_2022-07-0401_18_36.432528\\model-00043-0.01173-1.00000-0.45485-0.90000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0117 - categorical_accuracy: 1.0000 - val_loss: 0.4548 - val_categorical_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0139 - categorical_accuracy: 0.9985\n",
      "Epoch 44: saving model to model_init_2022-07-0401_18_36.432528\\model-00044-0.01385-0.99849-0.48687-0.87000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.0139 - categorical_accuracy: 0.9985 - val_loss: 0.4869 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0123 - categorical_accuracy: 1.0000\n",
      "Epoch 45: saving model to model_init_2022-07-0401_18_36.432528\\model-00045-0.01232-1.00000-0.45903-0.88000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.0123 - categorical_accuracy: 1.0000 - val_loss: 0.4590 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0107 - categorical_accuracy: 1.0000\n",
      "Epoch 46: saving model to model_init_2022-07-0401_18_36.432528\\model-00046-0.01066-1.00000-0.51964-0.87000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0107 - categorical_accuracy: 1.0000 - val_loss: 0.5196 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0100 - categorical_accuracy: 1.0000\n",
      "Epoch 47: saving model to model_init_2022-07-0401_18_36.432528\\model-00047-0.00997-1.00000-0.37075-0.90000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0100 - categorical_accuracy: 1.0000 - val_loss: 0.3708 - val_categorical_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0111 - categorical_accuracy: 1.0000\n",
      "Epoch 48: saving model to model_init_2022-07-0401_18_36.432528\\model-00048-0.01107-1.00000-0.49301-0.87000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0111 - categorical_accuracy: 1.0000 - val_loss: 0.4930 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0113 - categorical_accuracy: 1.0000\n",
      "Epoch 49: saving model to model_init_2022-07-0401_18_36.432528\\model-00049-0.01128-1.00000-0.44885-0.88000.h5\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.0113 - categorical_accuracy: 1.0000 - val_loss: 0.4489 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0110 - categorical_accuracy: 1.0000\n",
      "Epoch 50: saving model to model_init_2022-07-0401_18_36.432528\\model-00050-0.01104-1.00000-0.45372-0.87000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0110 - categorical_accuracy: 1.0000 - val_loss: 0.4537 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0116 - categorical_accuracy: 1.0000\n",
      "Epoch 51: saving model to model_init_2022-07-0401_18_36.432528\\model-00051-0.01157-1.00000-0.28951-0.93000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0116 - categorical_accuracy: 1.0000 - val_loss: 0.2895 - val_categorical_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0116 - categorical_accuracy: 1.0000\n",
      "Epoch 52: saving model to model_init_2022-07-0401_18_36.432528\\model-00052-0.01164-1.00000-0.43736-0.89000.h5\n",
      "21/21 [==============================] - 164s 8s/step - loss: 0.0116 - categorical_accuracy: 1.0000 - val_loss: 0.4374 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0112 - categorical_accuracy: 1.0000\n",
      "Epoch 53: saving model to model_init_2022-07-0401_18_36.432528\\model-00053-0.01116-1.00000-0.47353-0.88000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0112 - categorical_accuracy: 1.0000 - val_loss: 0.4735 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0118 - categorical_accuracy: 1.0000\n",
      "Epoch 54: saving model to model_init_2022-07-0401_18_36.432528\\model-00054-0.01184-1.00000-0.54511-0.85000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0118 - categorical_accuracy: 1.0000 - val_loss: 0.5451 - val_categorical_accuracy: 0.8500 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0105 - categorical_accuracy: 1.0000\n",
      "Epoch 55: saving model to model_init_2022-07-0401_18_36.432528\\model-00055-0.01054-1.00000-0.48260-0.85000.h5\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.0105 - categorical_accuracy: 1.0000 - val_loss: 0.4826 - val_categorical_accuracy: 0.8500 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0109 - categorical_accuracy: 1.0000\n",
      "Epoch 56: saving model to model_init_2022-07-0401_18_36.432528\\model-00056-0.01094-1.00000-0.46801-0.87000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0109 - categorical_accuracy: 1.0000 - val_loss: 0.4680 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0120 - categorical_accuracy: 1.0000\n",
      "Epoch 57: saving model to model_init_2022-07-0401_18_36.432528\\model-00057-0.01199-1.00000-0.47018-0.88000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0120 - categorical_accuracy: 1.0000 - val_loss: 0.4702 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0111 - categorical_accuracy: 1.0000\n",
      "Epoch 58: saving model to model_init_2022-07-0401_18_36.432528\\model-00058-0.01105-1.00000-0.39972-0.90000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0111 - categorical_accuracy: 1.0000 - val_loss: 0.3997 - val_categorical_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0111 - categorical_accuracy: 1.0000\n",
      "Epoch 59: saving model to model_init_2022-07-0401_18_36.432528\\model-00059-0.01108-1.00000-0.48986-0.87000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0111 - categorical_accuracy: 1.0000 - val_loss: 0.4899 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0099 - categorical_accuracy: 1.0000\n",
      "Epoch 60: saving model to model_init_2022-07-0401_18_36.432528\\model-00060-0.00994-1.00000-0.41913-0.89000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0099 - categorical_accuracy: 1.0000 - val_loss: 0.4191 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0105 - categorical_accuracy: 1.0000\n",
      "Epoch 61: saving model to model_init_2022-07-0401_18_36.432528\\model-00061-0.01052-1.00000-0.46728-0.88000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0105 - categorical_accuracy: 1.0000 - val_loss: 0.4673 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0092 - categorical_accuracy: 1.0000\n",
      "Epoch 62: saving model to model_init_2022-07-0401_18_36.432528\\model-00062-0.00919-1.00000-0.36280-0.91000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.3628 - val_categorical_accuracy: 0.9100 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0105 - categorical_accuracy: 1.0000\n",
      "Epoch 63: saving model to model_init_2022-07-0401_18_36.432528\\model-00063-0.01048-1.00000-0.42644-0.88000.h5\n",
      "21/21 [==============================] - 166s 8s/step - loss: 0.0105 - categorical_accuracy: 1.0000 - val_loss: 0.4264 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0096 - categorical_accuracy: 1.0000\n",
      "Epoch 64: saving model to model_init_2022-07-0401_18_36.432528\\model-00064-0.00955-1.00000-0.46788-0.88000.h5\n",
      "21/21 [==============================] - 153s 7s/step - loss: 0.0096 - categorical_accuracy: 1.0000 - val_loss: 0.4679 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0091 - categorical_accuracy: 1.0000\n",
      "Epoch 65: saving model to model_init_2022-07-0401_18_36.432528\\model-00065-0.00908-1.00000-0.47050-0.88000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0091 - categorical_accuracy: 1.0000 - val_loss: 0.4705 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0101 - categorical_accuracy: 1.0000\n",
      "Epoch 66: saving model to model_init_2022-07-0401_18_36.432528\\model-00066-0.01007-1.00000-0.55163-0.85000.h5\n",
      "21/21 [==============================] - 153s 7s/step - loss: 0.0101 - categorical_accuracy: 1.0000 - val_loss: 0.5516 - val_categorical_accuracy: 0.8500 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0101 - categorical_accuracy: 1.0000\n",
      "Epoch 67: saving model to model_init_2022-07-0401_18_36.432528\\model-00067-0.01008-1.00000-0.47560-0.87000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0101 - categorical_accuracy: 1.0000 - val_loss: 0.4756 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0084 - categorical_accuracy: 1.0000\n",
      "Epoch 68: saving model to model_init_2022-07-0401_18_36.432528\\model-00068-0.00842-1.00000-0.45676-0.88000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0084 - categorical_accuracy: 1.0000 - val_loss: 0.4568 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0092 - categorical_accuracy: 1.0000\n",
      "Epoch 69: saving model to model_init_2022-07-0401_18_36.432528\\model-00069-0.00918-1.00000-0.48727-0.88000.h5\n",
      "21/21 [==============================] - 160s 8s/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.4873 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0096 - categorical_accuracy: 1.0000\n",
      "Epoch 70: saving model to model_init_2022-07-0401_18_36.432528\\model-00070-0.00956-1.00000-0.40516-0.91000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0096 - categorical_accuracy: 1.0000 - val_loss: 0.4052 - val_categorical_accuracy: 0.9100 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0097 - categorical_accuracy: 1.0000\n",
      "Epoch 71: saving model to model_init_2022-07-0401_18_36.432528\\model-00071-0.00968-1.00000-0.67818-0.83000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0097 - categorical_accuracy: 1.0000 - val_loss: 0.6782 - val_categorical_accuracy: 0.8300 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0090 - categorical_accuracy: 1.0000\n",
      "Epoch 72: saving model to model_init_2022-07-0401_18_36.432528\\model-00072-0.00898-1.00000-0.48697-0.88000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0090 - categorical_accuracy: 1.0000 - val_loss: 0.4870 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0088 - categorical_accuracy: 1.0000\n",
      "Epoch 73: saving model to model_init_2022-07-0401_18_36.432528\\model-00073-0.00883-1.00000-0.48562-0.88000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0088 - categorical_accuracy: 1.0000 - val_loss: 0.4856 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0085 - categorical_accuracy: 1.0000\n",
      "Epoch 74: saving model to model_init_2022-07-0401_18_36.432528\\model-00074-0.00850-1.00000-0.37389-0.90000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0085 - categorical_accuracy: 1.0000 - val_loss: 0.3739 - val_categorical_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0082 - categorical_accuracy: 1.0000\n",
      "Epoch 75: saving model to model_init_2022-07-0401_18_36.432528\\model-00075-0.00822-1.00000-0.51674-0.89000.h5\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.0082 - categorical_accuracy: 1.0000 - val_loss: 0.5167 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0086 - categorical_accuracy: 1.0000\n",
      "Epoch 76: saving model to model_init_2022-07-0401_18_36.432528\\model-00076-0.00862-1.00000-0.48446-0.88000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.4845 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0089 - categorical_accuracy: 1.0000\n",
      "Epoch 77: saving model to model_init_2022-07-0401_18_36.432528\\model-00077-0.00888-1.00000-0.48398-0.88000.h5\n",
      "21/21 [==============================] - 153s 7s/step - loss: 0.0089 - categorical_accuracy: 1.0000 - val_loss: 0.4840 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0092 - categorical_accuracy: 1.0000\n",
      "Epoch 78: saving model to model_init_2022-07-0401_18_36.432528\\model-00078-0.00918-1.00000-0.55806-0.86000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.5581 - val_categorical_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0076 - categorical_accuracy: 1.0000\n",
      "Epoch 79: saving model to model_init_2022-07-0401_18_36.432528\\model-00079-0.00764-1.00000-0.36933-0.90000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0076 - categorical_accuracy: 1.0000 - val_loss: 0.3693 - val_categorical_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0085 - categorical_accuracy: 1.0000\n",
      "Epoch 80: saving model to model_init_2022-07-0401_18_36.432528\\model-00080-0.00852-1.00000-0.41900-0.89000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0085 - categorical_accuracy: 1.0000 - val_loss: 0.4190 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0093 - categorical_accuracy: 1.0000\n",
      "Epoch 81: saving model to model_init_2022-07-0401_18_36.432528\\model-00081-0.00930-1.00000-0.47691-0.88000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0093 - categorical_accuracy: 1.0000 - val_loss: 0.4769 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0072 - categorical_accuracy: 1.0000\n",
      "Epoch 82: saving model to model_init_2022-07-0401_18_36.432528\\model-00082-0.00721-1.00000-0.42800-0.89000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0072 - categorical_accuracy: 1.0000 - val_loss: 0.4280 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0083 - categorical_accuracy: 1.0000\n",
      "Epoch 83: saving model to model_init_2022-07-0401_18_36.432528\\model-00083-0.00832-1.00000-0.50262-0.89000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0083 - categorical_accuracy: 1.0000 - val_loss: 0.5026 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0086 - categorical_accuracy: 1.0000\n",
      "Epoch 84: saving model to model_init_2022-07-0401_18_36.432528\\model-00084-0.00857-1.00000-0.51612-0.88000.h5\n",
      "21/21 [==============================] - 155s 8s/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.5161 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0082 - categorical_accuracy: 1.0000\n",
      "Epoch 85: saving model to model_init_2022-07-0401_18_36.432528\\model-00085-0.00817-1.00000-0.47883-0.88000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0082 - categorical_accuracy: 1.0000 - val_loss: 0.4788 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0078 - categorical_accuracy: 1.0000\n",
      "Epoch 86: saving model to model_init_2022-07-0401_18_36.432528\\model-00086-0.00777-1.00000-0.58723-0.86000.h5\n",
      "21/21 [==============================] - 169s 8s/step - loss: 0.0078 - categorical_accuracy: 1.0000 - val_loss: 0.5872 - val_categorical_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0081 - categorical_accuracy: 1.0000\n",
      "Epoch 87: saving model to model_init_2022-07-0401_18_36.432528\\model-00087-0.00814-1.00000-0.48531-0.88000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 0.4853 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0094 - categorical_accuracy: 1.0000\n",
      "Epoch 88: saving model to model_init_2022-07-0401_18_36.432528\\model-00088-0.00937-1.00000-0.47228-0.88000.h5\n",
      "21/21 [==============================] - 151s 7s/step - loss: 0.0094 - categorical_accuracy: 1.0000 - val_loss: 0.4723 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0074 - categorical_accuracy: 1.0000\n",
      "Epoch 89: saving model to model_init_2022-07-0401_18_36.432528\\model-00089-0.00735-1.00000-0.50073-0.88000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0074 - categorical_accuracy: 1.0000 - val_loss: 0.5007 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0083 - categorical_accuracy: 1.0000\n",
      "Epoch 90: saving model to model_init_2022-07-0401_18_36.432528\\model-00090-0.00830-1.00000-0.61451-0.86000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0083 - categorical_accuracy: 1.0000 - val_loss: 0.6145 - val_categorical_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0085 - categorical_accuracy: 1.0000\n",
      "Epoch 91: saving model to model_init_2022-07-0401_18_36.432528\\model-00091-0.00852-1.00000-0.57787-0.87000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0085 - categorical_accuracy: 1.0000 - val_loss: 0.5779 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0082 - categorical_accuracy: 1.0000\n",
      "Epoch 92: saving model to model_init_2022-07-0401_18_36.432528\\model-00092-0.00817-1.00000-0.53942-0.87000.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 0.0082 - categorical_accuracy: 1.0000 - val_loss: 0.5394 - val_categorical_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0076 - categorical_accuracy: 1.0000\n",
      "Epoch 93: saving model to model_init_2022-07-0401_18_36.432528\\model-00093-0.00762-1.00000-0.49578-0.88000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0076 - categorical_accuracy: 1.0000 - val_loss: 0.4958 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0079 - categorical_accuracy: 1.0000\n",
      "Epoch 94: saving model to model_init_2022-07-0401_18_36.432528\\model-00094-0.00787-1.00000-0.45831-0.89000.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.0079 - categorical_accuracy: 1.0000 - val_loss: 0.4583 - val_categorical_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0068 - categorical_accuracy: 1.0000\n",
      "Epoch 95: saving model to model_init_2022-07-0401_18_36.432528\\model-00095-0.00685-1.00000-0.35806-0.90000.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.0068 - categorical_accuracy: 1.0000 - val_loss: 0.3581 - val_categorical_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0077 - categorical_accuracy: 1.0000\n",
      "Epoch 96: saving model to model_init_2022-07-0401_18_36.432528\\model-00096-0.00775-1.00000-0.51468-0.88000.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 0.0077 - categorical_accuracy: 1.0000 - val_loss: 0.5147 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0071 - categorical_accuracy: 1.0000\n",
      "Epoch 97: saving model to model_init_2022-07-0401_18_36.432528\\model-00097-0.00712-1.00000-0.49371-0.88000.h5\n",
      "21/21 [==============================] - 156s 8s/step - loss: 0.0071 - categorical_accuracy: 1.0000 - val_loss: 0.4937 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0086 - categorical_accuracy: 1.0000\n",
      "Epoch 98: saving model to model_init_2022-07-0401_18_36.432528\\model-00098-0.00859-1.00000-0.57536-0.86000.h5\n",
      "21/21 [==============================] - 159s 8s/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.5754 - val_categorical_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0066 - categorical_accuracy: 1.0000\n",
      "Epoch 99: saving model to model_init_2022-07-0401_18_36.432528\\model-00099-0.00658-1.00000-0.47985-0.88000.h5\n",
      "21/21 [==============================] - 160s 8s/step - loss: 0.0066 - categorical_accuracy: 1.0000 - val_loss: 0.4798 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0073 - categorical_accuracy: 1.0000\n",
      "Epoch 100: saving model to model_init_2022-07-0401_18_36.432528\\model-00100-0.00727-1.00000-0.49289-0.88000.h5\n",
      "21/21 [==============================] - 152s 7s/step - loss: 0.0073 - categorical_accuracy: 1.0000 - val_loss: 0.4929 - val_categorical_accuracy: 0.8800 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8f06e7880>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "          callbacks=callbacks_list, validation_data=val_generator, \n",
    "          validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5inOqvlo6ZJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Neural_Nets_Project_Gesture_Recognition.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
